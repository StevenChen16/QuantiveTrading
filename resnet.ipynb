{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08aa9bc-14dc-4f80-99cc-af872f5c4572",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CSV files:  10%|▉         | 500/5133 [00:05<00:48, 96.50it/s] \n",
      "Preprocessing data: 100%|██████████| 500/500 [03:22<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data shape: (2218500, 45, 16, 1)\n",
      "y_data shape: (2218500,)\n",
      "NaN in x_data: 0\n",
      "NaN in y_data: 0\n",
      "x_data shape after removing NaN: (2218500, 45, 16, 1)\n",
      "y_data shape after removing NaN: (2218500,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 加载所有CSV文件\n",
    "def load_data(data_folder):\n",
    "    data_frames = []\n",
    "    num = 0\n",
    "    for file in tqdm(os.listdir(data_folder), desc=\"Loading CSV files\"):\n",
    "        if num >= 500:\n",
    "            break\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(data_folder, file), index_col=0, parse_dates=True)\n",
    "            data_frames.append(df)\n",
    "        num += 1\n",
    "    return data_frames\n",
    "\n",
    "# 数据预处理\n",
    "def preprocess_data(df_list, time_window, future_window):\n",
    "    x_data, y_data = [], []\n",
    "    for df in tqdm(df_list, desc=\"Preprocessing data\"):\n",
    "        df = df[['open', 'close', 'high', 'low', 'volume', 'money', 'avg', 'high_limit', 'low_limit', 'pre_close', 'paused', 'factor', 'MA5', 'MA10', 'RSI', 'Williams %R']]\n",
    "        \n",
    "        # 处理 NaN 值\n",
    "        df = df.ffill().bfill()\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(df)\n",
    "        \n",
    "        for i in range(len(scaled_data) - time_window - future_window):\n",
    "            x_data.append(scaled_data[i:i + time_window])\n",
    "            future_close = df.iloc[i + time_window + future_window]['close']\n",
    "            current_close = df.iloc[i + time_window]['close']\n",
    "            y_data.append((future_close - current_close) / current_close)  # 涨跌幅度百分比\n",
    "\n",
    "    x_data = np.array(x_data)\n",
    "    y_data = np.array(y_data)\n",
    "    x_data = np.expand_dims(x_data, axis=-1)\n",
    "    return x_data, y_data\n",
    "\n",
    "# 检查数据加载和预处理部分\n",
    "data_folder = '/root/autodl-tmp/processed_data'  # 数据文件夹路径\n",
    "time_window = 45  # 时间窗口大小\n",
    "future_window = 10  # 预测未来多少天的涨跌幅度\n",
    "\n",
    "df_list = load_data(data_folder)\n",
    "x_data, y_data = preprocess_data(df_list, time_window, future_window)\n",
    "\n",
    "# 输出一些数据统计信息\n",
    "print(\"x_data shape:\", x_data.shape)\n",
    "print(\"y_data shape:\", y_data.shape)\n",
    "print(\"NaN in x_data:\", np.isnan(x_data).sum())\n",
    "print(\"NaN in y_data:\", np.isnan(y_data).sum())\n",
    "\n",
    "# 如果存在 NaN 值，处理掉\n",
    "if np.isnan(x_data).sum() > 0:\n",
    "    x_data = x_data[~np.isnan(x_data).any(axis=(1, 2, 3))]\n",
    "if np.isnan(y_data).sum() > 0:\n",
    "    y_data = y_data[~np.isnan(y_data)]\n",
    "\n",
    "print(\"x_data shape after removing NaN:\", x_data.shape)\n",
    "print(\"y_data shape after removing NaN:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a8422-f069-4e4d-8063-f5ddd88a055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_data.pkl', 'wb') as file:\n",
    "    pickle.dump(x_data, file)\n",
    "with open('y_data.pkl', 'wb') as file:\n",
    "    pickle.dump(y_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06680909-f4b8-47f9-a7ca-d86f739aa59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('x_data.pkl', 'rb') as file:\n",
    "    x_data = pickle.load(file)\n",
    "with open('y_data.pkl', 'rb') as file:\n",
    "    y_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e1e53b-4322-4567-9a02-9a7f8acbab51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 04:44:40.329265: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-20 04:44:40.387140: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 04:44:41.378827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-20 04:44:42.286707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.326583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.326990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.333209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.333577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.333875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.445258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.446673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.448059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-20 04:44:42.449467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22456 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0e:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 45, 16, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 45, 16, 32)           320       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 23, 8, 32)            0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 23, 8, 64)            18496     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 23, 8, 64)            256       ['conv2d_1[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 23, 8, 64)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 23, 8, 64)            36928     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 23, 8, 64)            2112      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 23, 8, 64)            256       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 23, 8, 64)            256       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 23, 8, 64)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 23, 8, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 4, 64)            0         ['activation_1[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 12, 4, 128)           73856     ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 12, 4, 128)           512       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 12, 4, 128)           0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 12, 4, 128)           147584    ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 12, 4, 128)           8320      ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 12, 4, 128)           512       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 12, 4, 128)           512       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 12, 4, 128)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    , 'batch_normalization_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 12, 4, 128)           0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 2, 128)            0         ['activation_3[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 6, 2, 256)            295168    ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 6, 2, 256)            1024      ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 6, 2, 256)            0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 6, 2, 256)            590080    ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 6, 2, 256)            33024     ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 6, 2, 256)            1024      ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 6, 2, 256)            1024      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 6, 2, 256)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 6, 2, 256)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 3, 1, 256)            0         ['activation_5[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 768)                  0         ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   49216     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    65        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1260545 (4.81 MB)\n",
      "Trainable params: 1257857 (4.80 MB)\n",
      "Non-trainable params: 2688 (10.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 残差块定义\n",
    "def residual_block(x, filters, kernel_size=3, stride=1, activation='relu'):\n",
    "    shortcut = x\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # 如果输入和输出的维度不同，通过卷积调整维度\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# 构建残差网络模型\n",
    "def build_resnet_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = residual_block(x, 128)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = residual_block(x, 256)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='linear')(x)  # 预测涨跌幅度\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "input_shape = x_data.shape[1:]\n",
    "model = build_resnet_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a06fc2-361d-488b-842c-1d7b76c22a70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1774800, 45, 16, 1)\n",
      "x_test shape: (443700, 45, 16, 1)\n",
      "y_train shape: (1774800,)\n",
      "y_test shape: (443700,)\n",
      "NaN in x_train: 0\n",
      "NaN in y_train: 0\n",
      "NaN in x_test: 0\n",
      "NaN in y_test: 0\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 04:45:13.621372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-06-20 04:45:14.150658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-06-20 04:45:14.173015: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ea8d5c9730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-20 04:45:14.173046: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-06-20 04:45:14.178856: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-20 04:45:14.323803: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27732/27732 [==============================] - 188s 6ms/step - loss: 0.0223 - mae: 0.0684 - val_loss: 0.0098 - val_mae: 0.0630\n",
      "Epoch 2/10\n",
      "27732/27732 [==============================] - 173s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0639\n",
      "Epoch 3/10\n",
      "27732/27732 [==============================] - 170s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0636\n",
      "Epoch 4/10\n",
      "27732/27732 [==============================] - 164s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0636\n",
      "Epoch 5/10\n",
      "27732/27732 [==============================] - 163s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0634\n",
      "Epoch 6/10\n",
      "27732/27732 [==============================] - 166s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0631\n",
      "Epoch 7/10\n",
      "27732/27732 [==============================] - 166s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0632\n",
      "Epoch 8/10\n",
      "27732/27732 [==============================] - 171s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0639\n",
      "Epoch 9/10\n",
      "27732/27732 [==============================] - 176s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0647\n",
      "Epoch 10/10\n",
      "27732/27732 [==============================] - 165s 6ms/step - loss: 0.0194 - mae: 0.0676 - val_loss: 0.0098 - val_mae: 0.0634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Can't synchronously write data (file write failed: time = Thu Jun 20 05:13:33 2024\n, filename = 'stock_prediction_resnet_model.h5', file descriptor = 89, errno = 28, error message = 'No space left on device', buf = 0x55ea94aa2870, total write size = 2331808, bytes this sub-write = 2331808, bytes actually written = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 训练模型并保存模型\u001b[39;00m\n\u001b[1;32m     18\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test))\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstock_prediction_resnet_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/h5py/_hl/dataset.py:999\u001b[0m, in \u001b[0;36mDataset.__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    997\u001b[0m mspace \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(selection\u001b[38;5;241m.\u001b[39mexpand_shape(mshape))\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fspace \u001b[38;5;129;01min\u001b[39;00m selection\u001b[38;5;241m.\u001b[39mbroadcast(mshape):\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:282\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_proxy.pyx:115\u001b[0m, in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] Can't synchronously write data (file write failed: time = Thu Jun 20 05:13:33 2024\n, filename = 'stock_prediction_resnet_model.h5', file descriptor = 89, errno = 28, error message = 'No space left on device', buf = 0x55ea94aa2870, total write size = 2331808, bytes this sub-write = 2331808, bytes actually written = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "# 数据分割\n",
    "split = int(0.8 * len(x_data))\n",
    "x_train, x_test = x_data[:split], x_data[split:]\n",
    "y_train, y_test = y_data[:split], y_data[split:]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# 检查训练数据和测试数据中是否存在NaN值\n",
    "print(\"NaN in x_train:\", np.isnan(x_train).sum())\n",
    "print(\"NaN in y_train:\", np.isnan(y_train).sum())\n",
    "print(\"NaN in x_test:\", np.isnan(x_test).sum())\n",
    "print(\"NaN in y_test:\", np.isnan(y_test).sum())\n",
    "\n",
    "# 训练模型并保存模型\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "model.save(\"stock_prediction_resnet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91474858-ccbe-4413-8bc1-24d2f8bd3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "test_loss, test_mae = model.evaluate(x_test, y_test)\n",
    "print(f\"测试损失: {test_loss}, 测试MAE: {test_mae}\")\n",
    "\n",
    "# 预测和可视化\n",
    "predictions = model.predict(x_test)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='真实涨跌幅度')\n",
    "plt.plot(predictions, label='预测涨跌幅度')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
